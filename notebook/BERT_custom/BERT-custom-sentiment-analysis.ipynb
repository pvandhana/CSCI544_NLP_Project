{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d3d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8501837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a237b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94f6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('IMDB_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa815e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d557fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2696d823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = labelEncoder.fit_transform(imdb_data['sentiment'])\n",
    "imdb_data.drop(labels=['sentiment'], axis=1, inplace=True)\n",
    "imdb_data['sentiment'] = encoded\n",
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d0f8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentiment\n",
       "count  50000.000000\n",
       "mean       0.500000\n",
       "std        0.500005\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307cba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3e2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85118322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12af3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fcf1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d61d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2885fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove HTML and URLs from comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10de78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review']\\\n",
    "    .replace('http\\S+', ' ', regex=True)\\\n",
    "    .replace('www\\S+', ' ', regex=True)\\\n",
    "    .replace('<\\S+>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745d13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f3790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review']\\\n",
    "    .replace('[^\\w +]', ' ', regex=True)\\\n",
    "    .replace(r'([\\;\\:\\|•«\\n])', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f57e54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove unnecessary spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83bb2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review'].str.strip()\n",
    "imdb_data['review'] = imdb_data['review']\\\n",
    "    .replace(' +', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfab444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resolving contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a34f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review']\\\n",
    "    .apply(lambda row: contractions.fix(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002b1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove digits and words with digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56ed0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review']\\\n",
    "    .replace('\\w*\\d\\w*', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22740167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Post data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dc3262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production br br the filmin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s love in the time of money is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i m going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      one of the other reviewers has mentioned that ...          1\n",
       "1      a wonderful little production br br the filmin...          1\n",
       "2      i thought this was a wonderful way to spend ti...          1\n",
       "3      basically there s a family where a little boy ...          0\n",
       "4      petter mattei s love in the time of money is a...          1\n",
       "...                                                  ...        ...\n",
       "49995  i thought this movie did a down right good job...          1\n",
       "49996  bad plot bad dialogue bad acting idiotic direc...          0\n",
       "49997  i am a catholic taught in parochial elementary...          0\n",
       "49998  i m going to have to disagree with the previou...          0\n",
       "49999  no one expects the star trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a065f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train - Valid - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c69e791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb, temp = train_test_split(imdb_data, train_size=0.80, shuffle=True)\n",
    "valid_imdb, test_imdb = train_test_split(temp, train_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f17538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n",
      "(5000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_imdb.shape)\n",
    "print(valid_imdb.shape)\n",
    "print(test_imdb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd08a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine Tuning BERT for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8a10fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91c1a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = self.bert.config.hidden_size, 50, 2\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb5291db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BertClassifier(freeze_bert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e307a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 350\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            truncation=True,                # Truncaate long sentences to max len\n",
    "            padding='max_length',\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7b44d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ce4db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(train_imdb['review'])\n",
    "val_inputs, val_masks = preprocessing_for_bert(valid_imdb['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5202e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(np.array(train_imdb['sentiment']))\n",
    "val_labels = torch.tensor(np.array(valid_imdb['sentiment']))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7848f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b729379",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d8bc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=2, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    min_error = math.inf\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.type(torch.LongTensor).to(device) for t in batch)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "        if val_loss <= min_error:\n",
    "            print('Min Error decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            min_error,\n",
    "            val_loss))\n",
    "            torch.save(model.state_dict(), 'BERT-new-best-saved-model.pt')\n",
    "            min_error = val_loss\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ff780f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.type(torch.LongTensor).to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af9347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7f2b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\swapn\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.699387   |     -      |     -     |   15.00  \n",
      "   1    |   40    |   0.710280   |     -      |     -     |   11.12  \n",
      "   1    |   60    |   0.684048   |     -      |     -     |   11.17  \n",
      "   1    |   80    |   0.563364   |     -      |     -     |   11.22  \n",
      "   1    |   100   |   0.595891   |     -      |     -     |   11.27  \n",
      "   1    |   120   |   0.452116   |     -      |     -     |   11.30  \n",
      "   1    |   140   |   0.542669   |     -      |     -     |   11.30  \n",
      "   1    |   160   |   0.340593   |     -      |     -     |   11.28  \n",
      "   1    |   180   |   0.481458   |     -      |     -     |   11.38  \n",
      "   1    |   200   |   0.406017   |     -      |     -     |   11.35  \n",
      "   1    |   220   |   0.477809   |     -      |     -     |   11.25  \n",
      "   1    |   240   |   0.523949   |     -      |     -     |   11.25  \n",
      "   1    |   260   |   0.342557   |     -      |     -     |   11.23  \n",
      "   1    |   280   |   0.476523   |     -      |     -     |   11.26  \n",
      "   1    |   300   |   0.394715   |     -      |     -     |   11.30  \n",
      "   1    |   320   |   0.370993   |     -      |     -     |   11.28  \n",
      "   1    |   340   |   0.412354   |     -      |     -     |   11.38  \n",
      "   1    |   360   |   0.389225   |     -      |     -     |   11.36  \n",
      "   1    |   380   |   0.394624   |     -      |     -     |   11.31  \n",
      "   1    |   400   |   0.332079   |     -      |     -     |   11.34  \n",
      "   1    |   420   |   0.435591   |     -      |     -     |   11.41  \n",
      "   1    |   440   |   0.478017   |     -      |     -     |   11.42  \n",
      "   1    |   460   |   0.435342   |     -      |     -     |   11.38  \n",
      "   1    |   480   |   0.302283   |     -      |     -     |   11.44  \n",
      "   1    |   500   |   0.412616   |     -      |     -     |   11.47  \n",
      "   1    |   520   |   0.414517   |     -      |     -     |   11.46  \n",
      "   1    |   540   |   0.372575   |     -      |     -     |   11.44  \n",
      "   1    |   560   |   0.232654   |     -      |     -     |   11.51  \n",
      "   1    |   580   |   0.413932   |     -      |     -     |   11.44  \n",
      "   1    |   600   |   0.364603   |     -      |     -     |   11.48  \n",
      "   1    |   620   |   0.607804   |     -      |     -     |   11.49  \n",
      "   1    |   640   |   0.294741   |     -      |     -     |   11.54  \n",
      "   1    |   660   |   0.332738   |     -      |     -     |   11.55  \n",
      "   1    |   680   |   0.390345   |     -      |     -     |   11.52  \n",
      "   1    |   700   |   0.299999   |     -      |     -     |   11.49  \n",
      "   1    |   720   |   0.289394   |     -      |     -     |   11.54  \n",
      "   1    |   740   |   0.408562   |     -      |     -     |   11.60  \n",
      "   1    |   760   |   0.396039   |     -      |     -     |   11.60  \n",
      "   1    |   780   |   0.353171   |     -      |     -     |   11.61  \n",
      "   1    |   800   |   0.300334   |     -      |     -     |   11.58  \n",
      "   1    |   820   |   0.307556   |     -      |     -     |   11.55  \n",
      "   1    |   840   |   0.375780   |     -      |     -     |   11.59  \n",
      "   1    |   860   |   0.327429   |     -      |     -     |   11.62  \n",
      "   1    |   880   |   0.433528   |     -      |     -     |   11.61  \n",
      "   1    |   900   |   0.346716   |     -      |     -     |   11.72  \n",
      "   1    |   920   |   0.272993   |     -      |     -     |   11.61  \n",
      "   1    |   940   |   0.263191   |     -      |     -     |   11.66  \n",
      "   1    |   960   |   0.407532   |     -      |     -     |   11.65  \n",
      "   1    |   980   |   0.460451   |     -      |     -     |   11.58  \n",
      "   1    |  1000   |   0.464619   |     -      |     -     |   11.67  \n",
      "   1    |  1020   |   0.463037   |     -      |     -     |   20.95  \n",
      "   1    |  1040   |   0.406235   |     -      |     -     |   23.61  \n",
      "   1    |  1060   |   0.245004   |     -      |     -     |   23.15  \n",
      "   1    |  1080   |   0.282335   |     -      |     -     |   22.87  \n",
      "   1    |  1100   |   0.435031   |     -      |     -     |   22.99  \n",
      "   1    |  1120   |   0.259245   |     -      |     -     |   22.90  \n",
      "   1    |  1140   |   0.353343   |     -      |     -     |   22.89  \n",
      "   1    |  1160   |   0.325912   |     -      |     -     |   22.75  \n",
      "   1    |  1180   |   0.465434   |     -      |     -     |   22.71  \n",
      "   1    |  1200   |   0.240819   |     -      |     -     |   22.56  \n",
      "   1    |  1220   |   0.279655   |     -      |     -     |   22.33  \n",
      "   1    |  1240   |   0.285063   |     -      |     -     |   22.38  \n",
      "   1    |  1260   |   0.296373   |     -      |     -     |   22.43  \n",
      "   1    |  1280   |   0.413173   |     -      |     -     |   22.38  \n",
      "   1    |  1300   |   0.286596   |     -      |     -     |   22.62  \n",
      "   1    |  1320   |   0.283069   |     -      |     -     |   22.68  \n",
      "   1    |  1340   |   0.345737   |     -      |     -     |   22.62  \n",
      "   1    |  1360   |   0.363752   |     -      |     -     |   22.85  \n",
      "   1    |  1380   |   0.514882   |     -      |     -     |   22.69  \n",
      "   1    |  1400   |   0.273908   |     -      |     -     |   22.56  \n",
      "   1    |  1420   |   0.296911   |     -      |     -     |   22.57  \n",
      "   1    |  1440   |   0.291217   |     -      |     -     |   22.52  \n",
      "   1    |  1460   |   0.366966   |     -      |     -     |   22.48  \n",
      "   1    |  1480   |   0.199599   |     -      |     -     |   22.32  \n",
      "   1    |  1500   |   0.303934   |     -      |     -     |   22.36  \n",
      "   1    |  1520   |   0.378374   |     -      |     -     |   22.32  \n",
      "   1    |  1540   |   0.301103   |     -      |     -     |   22.31  \n",
      "   1    |  1560   |   0.301269   |     -      |     -     |   22.36  \n",
      "   1    |  1580   |   0.332857   |     -      |     -     |   22.30  \n",
      "   1    |  1600   |   0.279248   |     -      |     -     |   22.25  \n",
      "   1    |  1620   |   0.308747   |     -      |     -     |   22.23  \n",
      "   1    |  1640   |   0.488117   |     -      |     -     |   22.32  \n",
      "   1    |  1660   |   0.303177   |     -      |     -     |   22.20  \n",
      "   1    |  1680   |   0.349425   |     -      |     -     |   22.27  \n",
      "   1    |  1700   |   0.350924   |     -      |     -     |   22.12  \n",
      "   1    |  1720   |   0.148411   |     -      |     -     |   22.16  \n",
      "   1    |  1740   |   0.458437   |     -      |     -     |   22.26  \n",
      "   1    |  1760   |   0.346494   |     -      |     -     |   22.01  \n",
      "   1    |  1780   |   0.236957   |     -      |     -     |   22.08  \n",
      "   1    |  1800   |   0.356043   |     -      |     -     |   22.03  \n",
      "   1    |  1820   |   0.349101   |     -      |     -     |   22.10  \n",
      "   1    |  1840   |   0.230547   |     -      |     -     |   22.08  \n",
      "   1    |  1860   |   0.421798   |     -      |     -     |   22.10  \n",
      "   1    |  1880   |   0.400533   |     -      |     -     |   22.14  \n",
      "   1    |  1900   |   0.318704   |     -      |     -     |   22.37  \n",
      "   1    |  1920   |   0.332075   |     -      |     -     |   22.61  \n",
      "   1    |  1940   |   0.226221   |     -      |     -     |   22.64  \n",
      "   1    |  1960   |   0.330836   |     -      |     -     |   22.50  \n",
      "   1    |  1980   |   0.290680   |     -      |     -     |   22.41  \n",
      "   1    |  2000   |   0.349577   |     -      |     -     |   22.42  \n",
      "   1    |  2020   |   0.291582   |     -      |     -     |   22.51  \n",
      "   1    |  2040   |   0.308110   |     -      |     -     |   22.69  \n",
      "   1    |  2060   |   0.359551   |     -      |     -     |   22.82  \n",
      "   1    |  2080   |   0.268639   |     -      |     -     |   22.66  \n",
      "   1    |  2100   |   0.240925   |     -      |     -     |   22.50  \n",
      "   1    |  2120   |   0.359856   |     -      |     -     |   22.51  \n",
      "   1    |  2140   |   0.385976   |     -      |     -     |   22.38  \n",
      "   1    |  2160   |   0.249493   |     -      |     -     |   22.39  \n",
      "   1    |  2180   |   0.314685   |     -      |     -     |   22.44  \n",
      "   1    |  2200   |   0.211745   |     -      |     -     |   22.38  \n",
      "   1    |  2220   |   0.411724   |     -      |     -     |   22.33  \n",
      "   1    |  2240   |   0.254897   |     -      |     -     |   22.15  \n",
      "   1    |  2260   |   0.228285   |     -      |     -     |   22.20  \n",
      "   1    |  2280   |   0.312769   |     -      |     -     |   22.21  \n",
      "   1    |  2300   |   0.432322   |     -      |     -     |   22.27  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  2320   |   0.225517   |     -      |     -     |   22.00  \n",
      "   1    |  2340   |   0.179775   |     -      |     -     |   21.96  \n",
      "   1    |  2360   |   0.336086   |     -      |     -     |   21.98  \n",
      "   1    |  2380   |   0.400798   |     -      |     -     |   22.08  \n",
      "   1    |  2400   |   0.291477   |     -      |     -     |   21.88  \n",
      "   1    |  2420   |   0.268629   |     -      |     -     |   21.93  \n",
      "   1    |  2440   |   0.300277   |     -      |     -     |   21.99  \n",
      "   1    |  2460   |   0.212890   |     -      |     -     |   22.03  \n",
      "   1    |  2480   |   0.420676   |     -      |     -     |   21.82  \n",
      "   1    |  2500   |   0.328386   |     -      |     -     |   21.96  \n",
      "   1    |  2520   |   0.385793   |     -      |     -     |   22.02  \n",
      "   1    |  2540   |   0.293414   |     -      |     -     |   21.93  \n",
      "   1    |  2560   |   0.270514   |     -      |     -     |   21.99  \n",
      "   1    |  2580   |   0.207275   |     -      |     -     |   21.91  \n",
      "   1    |  2600   |   0.399534   |     -      |     -     |  146.06  \n",
      "   1    |  2620   |   0.341872   |     -      |     -     |   32.78  \n",
      "   1    |  2640   |   0.247684   |     -      |     -     |   22.36  \n",
      "   1    |  2660   |   0.397253   |     -      |     -     |   22.66  \n",
      "   1    |  2680   |   0.485478   |     -      |     -     |   22.85  \n",
      "   1    |  2700   |   0.399327   |     -      |     -     |   22.64  \n",
      "   1    |  2720   |   0.256019   |     -      |     -     |   22.63  \n",
      "   1    |  2740   |   0.322976   |     -      |     -     |   22.61  \n",
      "   1    |  2760   |   0.301904   |     -      |     -     |   22.54  \n",
      "   1    |  2780   |   0.304257   |     -      |     -     |   22.53  \n",
      "   1    |  2800   |   0.352136   |     -      |     -     |   22.55  \n",
      "   1    |  2820   |   0.271851   |     -      |     -     |   22.60  \n",
      "   1    |  2840   |   0.253272   |     -      |     -     |   22.76  \n",
      "   1    |  2860   |   0.326991   |     -      |     -     |   22.61  \n",
      "   1    |  2880   |   0.325672   |     -      |     -     |   22.69  \n",
      "   1    |  2900   |   0.375036   |     -      |     -     |   22.95  \n",
      "   1    |  2920   |   0.375233   |     -      |     -     |   22.70  \n",
      "   1    |  2940   |   0.297896   |     -      |     -     |   22.84  \n",
      "   1    |  2960   |   0.278720   |     -      |     -     |   22.82  \n",
      "   1    |  2980   |   0.229681   |     -      |     -     |   22.65  \n",
      "   1    |  3000   |   0.187089   |     -      |     -     |   22.60  \n",
      "   1    |  3020   |   0.336931   |     -      |     -     |   22.50  \n",
      "   1    |  3040   |   0.358648   |     -      |     -     |   22.68  \n",
      "   1    |  3060   |   0.361424   |     -      |     -     |   22.51  \n",
      "   1    |  3080   |   0.347818   |     -      |     -     |   22.60  \n",
      "   1    |  3100   |   0.225872   |     -      |     -     |   22.47  \n",
      "   1    |  3120   |   0.338791   |     -      |     -     |   22.42  \n",
      "   1    |  3140   |   0.246763   |     -      |     -     |   22.45  \n",
      "   1    |  3160   |   0.248519   |     -      |     -     |   22.50  \n",
      "   1    |  3180   |   0.466980   |     -      |     -     |   22.39  \n",
      "   1    |  3200   |   0.220230   |     -      |     -     |   22.47  \n",
      "   1    |  3220   |   0.232788   |     -      |     -     |   22.47  \n",
      "   1    |  3240   |   0.305773   |     -      |     -     |   22.38  \n",
      "   1    |  3260   |   0.378253   |     -      |     -     |   22.35  \n",
      "   1    |  3280   |   0.479993   |     -      |     -     |   22.33  \n",
      "   1    |  3300   |   0.250405   |     -      |     -     |   22.39  \n",
      "   1    |  3320   |   0.217875   |     -      |     -     |   22.35  \n",
      "   1    |  3340   |   0.251078   |     -      |     -     |   22.41  \n",
      "   1    |  3360   |   0.192429   |     -      |     -     |   22.37  \n",
      "   1    |  3380   |   0.534249   |     -      |     -     |   22.37  \n",
      "   1    |  3400   |   0.566583   |     -      |     -     |   22.29  \n",
      "   1    |  3420   |   0.351871   |     -      |     -     |   22.29  \n",
      "   1    |  3440   |   0.373245   |     -      |     -     |   22.21  \n",
      "   1    |  3460   |   0.321402   |     -      |     -     |   22.27  \n",
      "   1    |  3480   |   0.350709   |     -      |     -     |   22.32  \n",
      "   1    |  3500   |   0.184496   |     -      |     -     |   22.34  \n",
      "   1    |  3520   |   0.319270   |     -      |     -     |   22.44  \n",
      "   1    |  3540   |   0.205069   |     -      |     -     |   22.36  \n",
      "   1    |  3560   |   0.329683   |     -      |     -     |   22.37  \n",
      "   1    |  3580   |   0.345360   |     -      |     -     |   22.34  \n",
      "   1    |  3600   |   0.264122   |     -      |     -     |   22.21  \n",
      "   1    |  3620   |   0.244967   |     -      |     -     |   22.16  \n",
      "   1    |  3640   |   0.154344   |     -      |     -     |   22.38  \n",
      "   1    |  3660   |   0.354520   |     -      |     -     |   22.25  \n",
      "   1    |  3680   |   0.275337   |     -      |     -     |   22.45  \n",
      "   1    |  3700   |   0.250937   |     -      |     -     |   22.20  \n",
      "   1    |  3720   |   0.433010   |     -      |     -     |   22.12  \n",
      "   1    |  3740   |   0.234565   |     -      |     -     |   22.24  \n",
      "   1    |  3760   |   0.216171   |     -      |     -     |   22.22  \n",
      "   1    |  3780   |   0.270634   |     -      |     -     |   22.20  \n",
      "   1    |  3800   |   0.240432   |     -      |     -     |   22.23  \n",
      "   1    |  3820   |   0.338399   |     -      |     -     |   22.05  \n",
      "   1    |  3840   |   0.295355   |     -      |     -     |   22.27  \n",
      "   1    |  3860   |   0.256958   |     -      |     -     |   22.33  \n",
      "   1    |  3880   |   0.325386   |     -      |     -     |   22.19  \n",
      "   1    |  3900   |   0.256372   |     -      |     -     |   22.23  \n",
      "   1    |  3920   |   0.279550   |     -      |     -     |   22.23  \n",
      "   1    |  3940   |   0.110428   |     -      |     -     |   22.14  \n",
      "   1    |  3960   |   0.222924   |     -      |     -     |   22.13  \n",
      "   1    |  3980   |   0.288462   |     -      |     -     |   22.19  \n",
      "   1    |  4000   |   0.198466   |     -      |     -     |   22.16  \n",
      "   1    |  4020   |   0.209167   |     -      |     -     |   22.20  \n",
      "   1    |  4040   |   0.377739   |     -      |     -     |   22.21  \n",
      "   1    |  4060   |   0.344273   |     -      |     -     |   22.30  \n",
      "   1    |  4080   |   0.095248   |     -      |     -     |   22.30  \n",
      "   1    |  4100   |   0.315836   |     -      |     -     |   22.20  \n",
      "   1    |  4120   |   0.407003   |     -      |     -     |   22.18  \n",
      "   1    |  4140   |   0.355375   |     -      |     -     |   22.24  \n",
      "   1    |  4160   |   0.297111   |     -      |     -     |   22.20  \n",
      "   1    |  4180   |   0.438512   |     -      |     -     |   22.39  \n",
      "   1    |  4200   |   0.304665   |     -      |     -     |   22.30  \n",
      "   1    |  4220   |   0.165237   |     -      |     -     |   22.25  \n",
      "   1    |  4240   |   0.294220   |     -      |     -     |   22.24  \n",
      "   1    |  4260   |   0.242927   |     -      |     -     |   22.22  \n",
      "   1    |  4280   |   0.352121   |     -      |     -     |   22.17  \n",
      "   1    |  4300   |   0.467400   |     -      |     -     |   22.15  \n",
      "   1    |  4320   |   0.305067   |     -      |     -     |   22.05  \n",
      "   1    |  4340   |   0.232295   |     -      |     -     |   22.08  \n",
      "   1    |  4360   |   0.328626   |     -      |     -     |   22.10  \n",
      "   1    |  4380   |   0.207522   |     -      |     -     |   22.07  \n",
      "   1    |  4400   |   0.339707   |     -      |     -     |   22.02  \n",
      "   1    |  4420   |   0.223216   |     -      |     -     |   22.06  \n",
      "   1    |  4440   |   0.342840   |     -      |     -     |   22.06  \n",
      "   1    |  4460   |   0.280666   |     -      |     -     |   22.10  \n",
      "   1    |  4480   |   0.274499   |     -      |     -     |   22.20  \n",
      "   1    |  4500   |   0.284669   |     -      |     -     |   22.10  \n",
      "   1    |  4520   |   0.320656   |     -      |     -     |   22.02  \n",
      "   1    |  4540   |   0.289748   |     -      |     -     |   22.09  \n",
      "   1    |  4560   |   0.325073   |     -      |     -     |   22.08  \n",
      "   1    |  4580   |   0.184648   |     -      |     -     |   21.97  \n",
      "   1    |  4600   |   0.248797   |     -      |     -     |   22.00  \n",
      "   1    |  4620   |   0.344821   |     -      |     -     |   22.16  \n",
      "   1    |  4640   |   0.367376   |     -      |     -     |   22.13  \n",
      "   1    |  4660   |   0.301722   |     -      |     -     |   22.08  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  4680   |   0.281184   |     -      |     -     |   22.16  \n",
      "   1    |  4700   |   0.257668   |     -      |     -     |   22.09  \n",
      "   1    |  4720   |   0.266685   |     -      |     -     |   21.94  \n",
      "   1    |  4740   |   0.314006   |     -      |     -     |   21.94  \n",
      "   1    |  4760   |   0.230425   |     -      |     -     |   22.15  \n",
      "   1    |  4780   |   0.267500   |     -      |     -     |   22.17  \n",
      "   1    |  4800   |   0.137867   |     -      |     -     |   22.07  \n",
      "   1    |  4820   |   0.256123   |     -      |     -     |   22.07  \n",
      "   1    |  4840   |   0.275214   |     -      |     -     |   22.22  \n",
      "   1    |  4860   |   0.326402   |     -      |     -     |   22.01  \n",
      "   1    |  4880   |   0.240917   |     -      |     -     |   22.02  \n",
      "   1    |  4900   |   0.292458   |     -      |     -     |   22.07  \n",
      "   1    |  4920   |   0.345489   |     -      |     -     |   22.10  \n",
      "   1    |  4940   |   0.412601   |     -      |     -     |   22.17  \n",
      "   1    |  4960   |   0.330483   |     -      |     -     |   22.09  \n",
      "   1    |  4980   |   0.286889   |     -      |     -     |   22.11  \n",
      "   1    |  4999   |   0.216741   |     -      |     -     |   20.90  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.328880   |  0.255329  |   92.30   |  5415.17 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Min Error decreased (inf --> 0.255329).  Saving model ...\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.081311   |     -      |     -     |   23.34  \n",
      "   2    |   40    |   0.204081   |     -      |     -     |   22.33  \n",
      "   2    |   60    |   0.130773   |     -      |     -     |   22.23  \n",
      "   2    |   80    |   0.143952   |     -      |     -     |   22.31  \n",
      "   2    |   100   |   0.301291   |     -      |     -     |   22.20  \n",
      "   2    |   120   |   0.177233   |     -      |     -     |   22.21  \n",
      "   2    |   140   |   0.223624   |     -      |     -     |   22.15  \n",
      "   2    |   160   |   0.204342   |     -      |     -     |   22.07  \n",
      "   2    |   180   |   0.159628   |     -      |     -     |   22.06  \n",
      "   2    |   200   |   0.201926   |     -      |     -     |   21.95  \n",
      "   2    |   220   |   0.120570   |     -      |     -     |   22.09  \n",
      "   2    |   240   |   0.120364   |     -      |     -     |   21.95  \n",
      "   2    |   260   |   0.126745   |     -      |     -     |   22.06  \n",
      "   2    |   280   |   0.212136   |     -      |     -     |   22.06  \n",
      "   2    |   300   |   0.129131   |     -      |     -     |   21.99  \n",
      "   2    |   320   |   0.152708   |     -      |     -     |   22.08  \n",
      "   2    |   340   |   0.251533   |     -      |     -     |   21.99  \n",
      "   2    |   360   |   0.214467   |     -      |     -     |   22.00  \n",
      "   2    |   380   |   0.283587   |     -      |     -     |   22.01  \n",
      "   2    |   400   |   0.113206   |     -      |     -     |   21.97  \n",
      "   2    |   420   |   0.077579   |     -      |     -     |   21.88  \n",
      "   2    |   440   |   0.125955   |     -      |     -     |   22.03  \n",
      "   2    |   460   |   0.322921   |     -      |     -     |   22.01  \n",
      "   2    |   480   |   0.284270   |     -      |     -     |   22.03  \n",
      "   2    |   500   |   0.250781   |     -      |     -     |   22.05  \n",
      "   2    |   520   |   0.222675   |     -      |     -     |   21.99  \n",
      "   2    |   540   |   0.272006   |     -      |     -     |   21.89  \n",
      "   2    |   560   |   0.189469   |     -      |     -     |   21.98  \n",
      "   2    |   580   |   0.129592   |     -      |     -     |   22.06  \n",
      "   2    |   600   |   0.238024   |     -      |     -     |   22.05  \n",
      "   2    |   620   |   0.261888   |     -      |     -     |   22.00  \n",
      "   2    |   640   |   0.325616   |     -      |     -     |   21.97  \n",
      "   2    |   660   |   0.194571   |     -      |     -     |   22.07  \n",
      "   2    |   680   |   0.070797   |     -      |     -     |   22.01  \n",
      "   2    |   700   |   0.092652   |     -      |     -     |   21.86  \n",
      "   2    |   720   |   0.314094   |     -      |     -     |   22.09  \n",
      "   2    |   740   |   0.139108   |     -      |     -     |   22.03  \n",
      "   2    |   760   |   0.188908   |     -      |     -     |   21.87  \n",
      "   2    |   780   |   0.321654   |     -      |     -     |   22.03  \n",
      "   2    |   800   |   0.131204   |     -      |     -     |   21.92  \n",
      "   2    |   820   |   0.090821   |     -      |     -     |   22.00  \n",
      "   2    |   840   |   0.027500   |     -      |     -     |   21.96  \n",
      "   2    |   860   |   0.226418   |     -      |     -     |   21.88  \n",
      "   2    |   880   |   0.272119   |     -      |     -     |   21.95  \n",
      "   2    |   900   |   0.161319   |     -      |     -     |   21.97  \n",
      "   2    |   920   |   0.191888   |     -      |     -     |   21.94  \n",
      "   2    |   940   |   0.222316   |     -      |     -     |   21.89  \n",
      "   2    |   960   |   0.249632   |     -      |     -     |   21.97  \n",
      "   2    |   980   |   0.174611   |     -      |     -     |   22.00  \n",
      "   2    |  1000   |   0.193131   |     -      |     -     |   22.01  \n",
      "   2    |  1020   |   0.140079   |     -      |     -     |   22.06  \n",
      "   2    |  1040   |   0.089320   |     -      |     -     |   21.90  \n",
      "   2    |  1060   |   0.221916   |     -      |     -     |   22.05  \n",
      "   2    |  1080   |   0.117637   |     -      |     -     |   21.89  \n",
      "   2    |  1100   |   0.116963   |     -      |     -     |   22.01  \n",
      "   2    |  1120   |   0.213322   |     -      |     -     |   22.06  \n",
      "   2    |  1140   |   0.153940   |     -      |     -     |   21.90  \n",
      "   2    |  1160   |   0.211577   |     -      |     -     |   22.01  \n",
      "   2    |  1180   |   0.169708   |     -      |     -     |   21.91  \n",
      "   2    |  1200   |   0.166788   |     -      |     -     |   21.88  \n",
      "   2    |  1220   |   0.202875   |     -      |     -     |   21.98  \n",
      "   2    |  1240   |   0.113352   |     -      |     -     |   21.86  \n",
      "   2    |  1260   |   0.044909   |     -      |     -     |   21.95  \n",
      "   2    |  1280   |   0.114872   |     -      |     -     |   21.95  \n",
      "   2    |  1300   |   0.136317   |     -      |     -     |   22.03  \n",
      "   2    |  1320   |   0.058533   |     -      |     -     |   21.97  \n",
      "   2    |  1340   |   0.176413   |     -      |     -     |   21.95  \n",
      "   2    |  1360   |   0.336719   |     -      |     -     |   21.94  \n",
      "   2    |  1380   |   0.163994   |     -      |     -     |   21.93  \n",
      "   2    |  1400   |   0.137787   |     -      |     -     |   22.00  \n",
      "   2    |  1420   |   0.091145   |     -      |     -     |   21.98  \n",
      "   2    |  1440   |   0.109990   |     -      |     -     |   22.09  \n",
      "   2    |  1460   |   0.267118   |     -      |     -     |   21.87  \n",
      "   2    |  1480   |   0.126036   |     -      |     -     |   21.90  \n",
      "   2    |  1500   |   0.289196   |     -      |     -     |   21.98  \n",
      "   2    |  1520   |   0.364101   |     -      |     -     |   21.90  \n",
      "   2    |  1540   |   0.126166   |     -      |     -     |   21.95  \n",
      "   2    |  1560   |   0.227537   |     -      |     -     |   21.87  \n",
      "   2    |  1580   |   0.267001   |     -      |     -     |   22.03  \n",
      "   2    |  1600   |   0.200861   |     -      |     -     |   21.98  \n",
      "   2    |  1620   |   0.122167   |     -      |     -     |   21.90  \n",
      "   2    |  1640   |   0.162718   |     -      |     -     |   22.06  \n",
      "   2    |  1660   |   0.161441   |     -      |     -     |   21.88  \n",
      "   2    |  1680   |   0.137702   |     -      |     -     |   21.92  \n",
      "   2    |  1700   |   0.170406   |     -      |     -     |   22.00  \n",
      "   2    |  1720   |   0.088280   |     -      |     -     |   21.90  \n",
      "   2    |  1740   |   0.217412   |     -      |     -     |   21.90  \n",
      "   2    |  1760   |   0.176586   |     -      |     -     |   21.93  \n",
      "   2    |  1780   |   0.252721   |     -      |     -     |   21.99  \n",
      "   2    |  1800   |   0.247193   |     -      |     -     |   21.90  \n",
      "   2    |  1820   |   0.109741   |     -      |     -     |   22.01  \n",
      "   2    |  1840   |   0.176573   |     -      |     -     |   21.89  \n",
      "   2    |  1860   |   0.148367   |     -      |     -     |   21.89  \n",
      "   2    |  1880   |   0.128549   |     -      |     -     |   21.86  \n",
      "   2    |  1900   |   0.134824   |     -      |     -     |   21.91  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  1920   |   0.234328   |     -      |     -     |   21.84  \n",
      "   2    |  1940   |   0.121158   |     -      |     -     |   21.84  \n",
      "   2    |  1960   |   0.110569   |     -      |     -     |   21.91  \n",
      "   2    |  1980   |   0.134302   |     -      |     -     |   21.90  \n",
      "   2    |  2000   |   0.420544   |     -      |     -     |   21.92  \n",
      "   2    |  2020   |   0.171808   |     -      |     -     |   21.93  \n",
      "   2    |  2040   |   0.172926   |     -      |     -     |   21.96  \n",
      "   2    |  2060   |   0.227374   |     -      |     -     |   21.99  \n",
      "   2    |  2080   |   0.240127   |     -      |     -     |   22.02  \n",
      "   2    |  2100   |   0.161330   |     -      |     -     |   21.92  \n",
      "   2    |  2120   |   0.124510   |     -      |     -     |   21.96  \n",
      "   2    |  2140   |   0.321210   |     -      |     -     |   21.94  \n",
      "   2    |  2160   |   0.087276   |     -      |     -     |   21.87  \n",
      "   2    |  2180   |   0.132115   |     -      |     -     |   21.91  \n",
      "   2    |  2200   |   0.304824   |     -      |     -     |   22.01  \n",
      "   2    |  2220   |   0.172238   |     -      |     -     |   21.93  \n",
      "   2    |  2240   |   0.176158   |     -      |     -     |   21.97  \n",
      "   2    |  2260   |   0.160198   |     -      |     -     |   21.93  \n",
      "   2    |  2280   |   0.141133   |     -      |     -     |   21.82  \n",
      "   2    |  2300   |   0.082300   |     -      |     -     |   21.86  \n",
      "   2    |  2320   |   0.092910   |     -      |     -     |   21.88  \n",
      "   2    |  2340   |   0.177482   |     -      |     -     |   21.84  \n",
      "   2    |  2360   |   0.076863   |     -      |     -     |   21.88  \n",
      "   2    |  2380   |   0.095887   |     -      |     -     |   21.88  \n",
      "   2    |  2400   |   0.298073   |     -      |     -     |   21.80  \n",
      "   2    |  2420   |   0.204482   |     -      |     -     |   21.90  \n",
      "   2    |  2440   |   0.134716   |     -      |     -     |   21.87  \n",
      "   2    |  2460   |   0.189935   |     -      |     -     |   21.83  \n",
      "   2    |  2480   |   0.094434   |     -      |     -     |   21.98  \n",
      "   2    |  2500   |   0.111083   |     -      |     -     |   21.98  \n",
      "   2    |  2520   |   0.211137   |     -      |     -     |   21.89  \n",
      "   2    |  2540   |   0.217752   |     -      |     -     |   21.91  \n",
      "   2    |  2560   |   0.233740   |     -      |     -     |   21.93  \n",
      "   2    |  2580   |   0.228243   |     -      |     -     |   21.91  \n",
      "   2    |  2600   |   0.183427   |     -      |     -     |   21.88  \n",
      "   2    |  2620   |   0.177502   |     -      |     -     |   21.94  \n",
      "   2    |  2640   |   0.328565   |     -      |     -     |   21.86  \n",
      "   2    |  2660   |   0.210543   |     -      |     -     |   21.89  \n",
      "   2    |  2680   |   0.125037   |     -      |     -     |   21.96  \n",
      "   2    |  2700   |   0.070577   |     -      |     -     |   21.96  \n",
      "   2    |  2720   |   0.242730   |     -      |     -     |   21.85  \n",
      "   2    |  2740   |   0.254853   |     -      |     -     |   21.93  \n",
      "   2    |  2760   |   0.125105   |     -      |     -     |   21.87  \n",
      "   2    |  2780   |   0.253809   |     -      |     -     |   21.86  \n",
      "   2    |  2800   |   0.306199   |     -      |     -     |   21.86  \n",
      "   2    |  2820   |   0.156404   |     -      |     -     |   21.92  \n",
      "   2    |  2840   |   0.109486   |     -      |     -     |   21.83  \n",
      "   2    |  2860   |   0.167277   |     -      |     -     |   21.79  \n",
      "   2    |  2880   |   0.197842   |     -      |     -     |   21.99  \n",
      "   2    |  2900   |   0.280212   |     -      |     -     |   21.98  \n",
      "   2    |  2920   |   0.117805   |     -      |     -     |   22.04  \n",
      "   2    |  2940   |   0.240214   |     -      |     -     |   21.85  \n",
      "   2    |  2960   |   0.203815   |     -      |     -     |   21.94  \n",
      "   2    |  2980   |   0.204701   |     -      |     -     |   21.85  \n",
      "   2    |  3000   |   0.141215   |     -      |     -     |   21.85  \n",
      "   2    |  3020   |   0.099073   |     -      |     -     |   21.85  \n",
      "   2    |  3040   |   0.134243   |     -      |     -     |   21.98  \n",
      "   2    |  3060   |   0.213584   |     -      |     -     |   21.90  \n",
      "   2    |  3080   |   0.129689   |     -      |     -     |   21.91  \n",
      "   2    |  3100   |   0.279237   |     -      |     -     |   21.84  \n",
      "   2    |  3120   |   0.225770   |     -      |     -     |   21.86  \n",
      "   2    |  3140   |   0.117101   |     -      |     -     |   21.85  \n",
      "   2    |  3160   |   0.067664   |     -      |     -     |   21.84  \n",
      "   2    |  3180   |   0.217476   |     -      |     -     |   21.91  \n",
      "   2    |  3200   |   0.158022   |     -      |     -     |   21.93  \n",
      "   2    |  3220   |   0.168638   |     -      |     -     |   21.78  \n",
      "   2    |  3240   |   0.146915   |     -      |     -     |   21.84  \n",
      "   2    |  3260   |   0.104775   |     -      |     -     |   21.86  \n",
      "   2    |  3280   |   0.238135   |     -      |     -     |   21.98  \n",
      "   2    |  3300   |   0.155983   |     -      |     -     |   21.95  \n",
      "   2    |  3320   |   0.258359   |     -      |     -     |   21.85  \n",
      "   2    |  3340   |   0.108831   |     -      |     -     |   21.87  \n",
      "   2    |  3360   |   0.047383   |     -      |     -     |   21.90  \n",
      "   2    |  3380   |   0.260846   |     -      |     -     |   21.92  \n",
      "   2    |  3400   |   0.129876   |     -      |     -     |   21.87  \n",
      "   2    |  3420   |   0.147554   |     -      |     -     |   21.91  \n",
      "   2    |  3440   |   0.307718   |     -      |     -     |   21.82  \n",
      "   2    |  3460   |   0.228478   |     -      |     -     |   21.84  \n",
      "   2    |  3480   |   0.119996   |     -      |     -     |   21.93  \n",
      "   2    |  3500   |   0.248434   |     -      |     -     |   21.88  \n",
      "   2    |  3520   |   0.235161   |     -      |     -     |   21.84  \n",
      "   2    |  3540   |   0.121747   |     -      |     -     |   21.89  \n",
      "   2    |  3560   |   0.103141   |     -      |     -     |   21.98  \n",
      "   2    |  3580   |   0.246208   |     -      |     -     |   21.92  \n",
      "   2    |  3600   |   0.111480   |     -      |     -     |   21.80  \n",
      "   2    |  3620   |   0.108166   |     -      |     -     |   21.97  \n",
      "   2    |  3640   |   0.208986   |     -      |     -     |   21.89  \n",
      "   2    |  3660   |   0.167555   |     -      |     -     |   21.88  \n",
      "   2    |  3680   |   0.268966   |     -      |     -     |   21.94  \n",
      "   2    |  3700   |   0.180205   |     -      |     -     |   21.81  \n",
      "   2    |  3720   |   0.176297   |     -      |     -     |   21.82  \n",
      "   2    |  3740   |   0.174014   |     -      |     -     |   21.88  \n",
      "   2    |  3760   |   0.249478   |     -      |     -     |   21.95  \n",
      "   2    |  3780   |   0.145015   |     -      |     -     |   21.90  \n",
      "   2    |  3800   |   0.122132   |     -      |     -     |   21.77  \n",
      "   2    |  3820   |   0.340461   |     -      |     -     |   22.05  \n",
      "   2    |  3840   |   0.214473   |     -      |     -     |   21.92  \n",
      "   2    |  3860   |   0.164994   |     -      |     -     |   21.88  \n",
      "   2    |  3880   |   0.124421   |     -      |     -     |   21.97  \n",
      "   2    |  3900   |   0.187073   |     -      |     -     |   21.85  \n",
      "   2    |  3920   |   0.172380   |     -      |     -     |   21.92  \n",
      "   2    |  3940   |   0.130625   |     -      |     -     |   21.94  \n",
      "   2    |  3960   |   0.240832   |     -      |     -     |   21.89  \n",
      "   2    |  3980   |   0.243944   |     -      |     -     |   21.86  \n",
      "   2    |  4000   |   0.183477   |     -      |     -     |   21.84  \n",
      "   2    |  4020   |   0.192697   |     -      |     -     |   21.93  \n",
      "   2    |  4040   |   0.141748   |     -      |     -     |   21.82  \n",
      "   2    |  4060   |   0.108845   |     -      |     -     |   21.76  \n",
      "   2    |  4080   |   0.203213   |     -      |     -     |   21.79  \n",
      "   2    |  4100   |   0.090853   |     -      |     -     |   21.89  \n",
      "   2    |  4120   |   0.187564   |     -      |     -     |   21.81  \n",
      "   2    |  4140   |   0.215630   |     -      |     -     |   21.89  \n",
      "   2    |  4160   |   0.005881   |     -      |     -     |   21.82  \n",
      "   2    |  4180   |   0.294165   |     -      |     -     |   21.91  \n",
      "   2    |  4200   |   0.158822   |     -      |     -     |   21.85  \n",
      "   2    |  4220   |   0.159363   |     -      |     -     |   21.88  \n",
      "   2    |  4240   |   0.203403   |     -      |     -     |   21.97  \n",
      "   2    |  4260   |   0.133236   |     -      |     -     |   21.73  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  4280   |   0.103932   |     -      |     -     |   21.83  \n",
      "   2    |  4300   |   0.145844   |     -      |     -     |   21.80  \n",
      "   2    |  4320   |   0.128802   |     -      |     -     |   21.84  \n",
      "   2    |  4340   |   0.187283   |     -      |     -     |   21.85  \n",
      "   2    |  4360   |   0.135049   |     -      |     -     |   21.81  \n",
      "   2    |  4380   |   0.150776   |     -      |     -     |   21.77  \n",
      "   2    |  4400   |   0.101563   |     -      |     -     |   21.86  \n",
      "   2    |  4420   |   0.123992   |     -      |     -     |   21.90  \n",
      "   2    |  4440   |   0.239859   |     -      |     -     |   21.95  \n",
      "   2    |  4460   |   0.175891   |     -      |     -     |   21.81  \n",
      "   2    |  4480   |   0.120384   |     -      |     -     |   21.89  \n",
      "   2    |  4500   |   0.087169   |     -      |     -     |   21.84  \n",
      "   2    |  4520   |   0.144038   |     -      |     -     |   21.84  \n",
      "   2    |  4540   |   0.087797   |     -      |     -     |   21.89  \n",
      "   2    |  4560   |   0.281394   |     -      |     -     |   21.96  \n",
      "   2    |  4580   |   0.191862   |     -      |     -     |   21.74  \n",
      "   2    |  4600   |   0.147520   |     -      |     -     |   21.80  \n",
      "   2    |  4620   |   0.224288   |     -      |     -     |   21.90  \n",
      "   2    |  4640   |   0.118042   |     -      |     -     |   21.90  \n",
      "   2    |  4660   |   0.202039   |     -      |     -     |   21.87  \n",
      "   2    |  4680   |   0.162193   |     -      |     -     |   21.82  \n",
      "   2    |  4700   |   0.133078   |     -      |     -     |   21.91  \n",
      "   2    |  4720   |   0.087472   |     -      |     -     |   21.78  \n",
      "   2    |  4740   |   0.111051   |     -      |     -     |   21.83  \n",
      "   2    |  4760   |   0.097941   |     -      |     -     |   21.94  \n",
      "   2    |  4780   |   0.102168   |     -      |     -     |   21.79  \n",
      "   2    |  4800   |   0.084780   |     -      |     -     |   22.02  \n",
      "   2    |  4820   |   0.070858   |     -      |     -     |   21.90  \n",
      "   2    |  4840   |   0.187234   |     -      |     -     |   21.91  \n",
      "   2    |  4860   |   0.269906   |     -      |     -     |   21.97  \n",
      "   2    |  4880   |   0.103888   |     -      |     -     |   21.91  \n",
      "   2    |  4900   |   0.121845   |     -      |     -     |   21.94  \n",
      "   2    |  4920   |   0.284145   |     -      |     -     |   21.87  \n",
      "   2    |  4940   |   0.258814   |     -      |     -     |   21.94  \n",
      "   2    |  4960   |   0.119883   |     -      |     -     |   21.81  \n",
      "   2    |  4980   |   0.181727   |     -      |     -     |   21.90  \n",
      "   2    |  4999   |   0.175991   |     -      |     -     |   20.71  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.175775   |  0.293965  |   93.30   |  5722.50 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76dc8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed708410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5b38c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07bee3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_imdb['review'])\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d7d6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8e26946",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff74e5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_imdb['sentiment'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac88168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
